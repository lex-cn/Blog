


岭回归与Lasso回归的出现是为了解决线性回归出现的过拟合以及在通过正规方程方法求解$\theta$的过程中出现的$(X^TX)$不可逆这两类问题的，这两种回归均通过在损失函数中引入正则化项来达到目的。

在日常机器学习任务中，如果数据集的特征比样本点还多， $(X^TX)^{-1}$的时候会出错。岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入$\lambda$限制了所有$\theta^2$之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学上也叫作缩减（shrinkage）。和岭回归类似，另一个缩减LASSO 也加入了正则项对回归系数做了限定。

为了防止过拟合($\theta$过大)，在目标函数$J(\theta)$后添加复杂度惩罚因子，即正则项来防止过拟合。正则项可以使用L1-norm(Lasso)、L2-norm(Ridge)，或结合L1-norm、L2-norm(Elastic Net)。

Lasso：使用L1-norm正则

$$J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda \sum_{j}^{n}|\theta_j|$$

Ridge：使用L2-norm正则

$$J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda \sum_{j}^{n}\theta_j^2$$

ElasticNet：结合l1-norm、l2-norm进行正则

$$J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta ^Tx^{(i)})^2+\lambda (\rho\sum_{j}^{n}|\theta_j|+(1-\rho)\sum_{j}^{n}\theta_j^2)$$

简单的理解正则化：

正则化的目的：防止过拟合
正则化的本质：约束（限制）要优化的参数

关于第1点，过拟合指的是给定一堆数据，这堆数据带有噪声，利用模型去拟合这堆数据，可能会把噪声数据也给拟合了，这点很致命，一方面会造成模型比较复杂，另一方面，模型的泛化性能太差了，遇到了新的数据让你测试，你所得到的过拟合的模型，正确率是很差的。

关于第2点，本来解空间是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。这一点不得不提到一个图，相信我们都经常看到这个图，但貌似还没有一个特别清晰的解释，这里我尝试解释一下，图如下：

![](./2020-04-07-岭回归岭估计/2020-08-03-17-17-52.png)

## 参考资料

- [ElasticNet算法解析_qq_29462849的博客-CSDN博客_elasticnet](https://blog.csdn.net/qq_29462849/article/details/90581164)
- [从Lasso开始说起 - 知乎](https://zhuanlan.zhihu.com/p/46999826)
- [机器学习算法之岭回归、Lasso回归和ElasticNet回归 – 标点符](https://www.biaodianfu.com/ridge-lasso-elasticnet.html)
